{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48febecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main \n",
    "!pip install -q git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e020b",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3891ab0",
   "metadata": {},
   "source": [
    "- bigscience/bloom-7b1\n",
    "- lora fine-tune bloom: 可插拔式的（plugin/adapter）\n",
    "    - freeeze original weights\n",
    "    - plugin lora adapters (peft)\n",
    "- huggingface transformers 库\n",
    "    - trainer.train 的参数及过程；\n",
    "    - mlm 与 clm 的差异：（都是 unsupervised learning，都可以自动地构建 input/labels）\n",
    "        - mlm：bert\n",
    "        - clm：gpt（bloom）\n",
    "    - pipeline\n",
    "        - dataset/tasks\n",
    "        - tokenizer\n",
    "        - training (fine-tune base lora)\n",
    "        - inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dd11d",
   "metadata": {},
   "source": [
    "## base model & lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3acc67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T21:05:01.606705800Z",
     "start_time": "2023-11-05T21:05:01.388702300Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'bitsandbytes' has no attribute 'nn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoConfig, AutoModelForCausalLM\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraConfig, get_peft_model \n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\__init__.py:22\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.7.0.dev0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     23\u001B[0m     AutoPeftModel,\n\u001B[0;32m     24\u001B[0m     AutoPeftModelForCausalLM,\n\u001B[0;32m     25\u001B[0m     AutoPeftModelForSequenceClassification,\n\u001B[0;32m     26\u001B[0m     AutoPeftModelForSeq2SeqLM,\n\u001B[0;32m     27\u001B[0m     AutoPeftModelForTokenClassification,\n\u001B[0;32m     28\u001B[0m     AutoPeftModelForQuestionAnswering,\n\u001B[0;32m     29\u001B[0m     AutoPeftModelForFeatureExtraction,\n\u001B[0;32m     30\u001B[0m )\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     32\u001B[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001B[0;32m     33\u001B[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m     inject_adapter_in_model,\n\u001B[0;32m     37\u001B[0m )\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     PeftModel,\n\u001B[0;32m     40\u001B[0m     PeftModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     45\u001B[0m     PeftModelForFeatureExtraction,\n\u001B[0;32m     46\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\auto.py:31\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     22\u001B[0m     AutoModel,\n\u001B[0;32m     23\u001B[0m     AutoModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     27\u001B[0m     AutoModelForTokenClassification,\n\u001B[0;32m     28\u001B[0m )\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     33\u001B[0m     PeftModel,\n\u001B[0;32m     34\u001B[0m     PeftModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m     PeftModelForTokenClassification,\n\u001B[0;32m     40\u001B[0m )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_BaseAutoPeftModel\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\mapping.py:23\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     24\u001B[0m     PeftModel,\n\u001B[0;32m     25\u001B[0m     PeftModelForCausalLM,\n\u001B[0;32m     26\u001B[0m     PeftModelForFeatureExtraction,\n\u001B[0;32m     27\u001B[0m     PeftModelForQuestionAnswering,\n\u001B[0;32m     28\u001B[0m     PeftModelForSeq2SeqLM,\n\u001B[0;32m     29\u001B[0m     PeftModelForSequenceClassification,\n\u001B[0;32m     30\u001B[0m     PeftModelForTokenClassification,\n\u001B[0;32m     31\u001B[0m )\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtuners\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     33\u001B[0m     AdaLoraConfig,\n\u001B[0;32m     34\u001B[0m     AdaLoraModel,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m     PromptTuningConfig,\n\u001B[0;32m     48\u001B[0m )\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _prepare_prompt_learning_config\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\peft_model.py:38\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtuners\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     AdaLoraModel,\n\u001B[0;32m     40\u001B[0m     AdaptionPromptModel,\n\u001B[0;32m     41\u001B[0m     IA3Model,\n\u001B[0;32m     42\u001B[0m     LoHaModel,\n\u001B[0;32m     43\u001B[0m     LoKrModel,\n\u001B[0;32m     44\u001B[0m     LoraModel,\n\u001B[0;32m     45\u001B[0m     MultitaskPromptEmbedding,\n\u001B[0;32m     46\u001B[0m     PrefixEncoder,\n\u001B[0;32m     47\u001B[0m     PromptEmbedding,\n\u001B[0;32m     48\u001B[0m     PromptEncoder,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     51\u001B[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001B[0;32m     52\u001B[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     shift_tokens_right,\n\u001B[0;32m     65\u001B[0m )\n\u001B[0;32m     68\u001B[0m PEFT_TYPE_TO_MODEL_MAPPING \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     69\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mLORA: LoraModel,\n\u001B[0;32m     70\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mLOHA: LoHaModel,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     77\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mIA3: IA3Model,\n\u001B[0;32m     78\u001B[0m }\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\__init__.py:21\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madaption_prompt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlora\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraConfig, LoraModel\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloha\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoHaConfig, LoHaModel\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlokr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoKrConfig, LoKrModel\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py:21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgptq\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantLinear\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2d, Embedding, Linear, LoraLayer\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraModel\n\u001B[0;32m     24\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConv2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraLayer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraModel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantLinear\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_available():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\lora\\model.py:47\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_available():\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbnb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Linear8bitLt\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_4bit_available():\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbnb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Linear4bit\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:173\u001B[0m\n\u001B[0;32m    168\u001B[0m                     result \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m output\n\u001B[0;32m    170\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m--> 173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mis_bnb_4bit_available\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLinear4bit\u001B[39;00m(torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, LoraLayer):\n\u001B[0;32m    176\u001B[0m         \u001B[38;5;66;03m# Lora implemented in a dense layer\u001B[39;00m\n\u001B[0;32m    177\u001B[0m         \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    178\u001B[0m             \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    179\u001B[0m             adapter_name,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    184\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    185\u001B[0m         ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\import_utils.py:28\u001B[0m, in \u001B[0;36mis_bnb_4bit_available\u001B[1;34m()\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLinear4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'bitsandbytes' has no attribute 'nn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb \n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43585dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T11:29:04.743132Z",
     "start_time": "2023-06-04T11:29:04.728571Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db90920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T11:29:05.907887Z",
     "start_time": "2023-06-04T11:29:05.897714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.0.0+cu118\n",
      "bitsandbytes: 0.38.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3dd7c1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T20:52:17.675482700Z",
     "start_time": "2023-11-05T20:52:17.398769600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('D')}\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('module'), WindowsPath('/matplotlib_inline.backend_inline')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=117, Highest Compute Capability: 8.6.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Loading binary D:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwatermark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m watermark\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mwatermark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpackages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpeft,torch,loralib,transformers,accelerate,datasets\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\watermark\\watermark.py:188\u001B[0m, in \u001B[0;36mwatermark\u001B[1;34m(author, email, github_username, website, current_date, datename, current_time, iso8601, timezone, updated, custom_time, python, packages, conda, hostname, machine, githash, gitrepo, gitbranch, watermark, iversions, gpu, watermark_self, globals_)\u001B[0m\n\u001B[0;32m    186\u001B[0m     output\u001B[38;5;241m.\u001B[39mappend(_get_pyversions())\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpackages\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m--> 188\u001B[0m     output\u001B[38;5;241m.\u001B[39mappend(\u001B[43m_get_packages\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpackages\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconda\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m    190\u001B[0m     output\u001B[38;5;241m.\u001B[39mappend(_get_conda_env())\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\watermark\\watermark.py:243\u001B[0m, in \u001B[0;36m_get_packages\u001B[1;34m(pkgs)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_packages\u001B[39m(pkgs):\n\u001B[0;32m    242\u001B[0m     packages \u001B[38;5;241m=\u001B[39m pkgs\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {package: _get_package_version(package)\n\u001B[0;32m    244\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m package \u001B[38;5;129;01min\u001B[39;00m packages}\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\watermark\\watermark.py:243\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_packages\u001B[39m(pkgs):\n\u001B[0;32m    242\u001B[0m     packages \u001B[38;5;241m=\u001B[39m pkgs\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {package: \u001B[43m_get_package_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    244\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m package \u001B[38;5;129;01min\u001B[39;00m packages}\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\watermark\\watermark.py:252\u001B[0m, in \u001B[0;36m_get_package_version\u001B[1;34m(pkg_name)\u001B[0m\n\u001B[0;32m    250\u001B[0m     pkg_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msklearn\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m     imported \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpkg_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m    254\u001B[0m     version \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot installed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\importlib\\__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1014\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:991\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:975\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:671\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:843\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:219\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\__init__.py:22\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.7.0.dev0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     23\u001B[0m     AutoPeftModel,\n\u001B[0;32m     24\u001B[0m     AutoPeftModelForCausalLM,\n\u001B[0;32m     25\u001B[0m     AutoPeftModelForSequenceClassification,\n\u001B[0;32m     26\u001B[0m     AutoPeftModelForSeq2SeqLM,\n\u001B[0;32m     27\u001B[0m     AutoPeftModelForTokenClassification,\n\u001B[0;32m     28\u001B[0m     AutoPeftModelForQuestionAnswering,\n\u001B[0;32m     29\u001B[0m     AutoPeftModelForFeatureExtraction,\n\u001B[0;32m     30\u001B[0m )\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     32\u001B[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001B[0;32m     33\u001B[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m     inject_adapter_in_model,\n\u001B[0;32m     37\u001B[0m )\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     PeftModel,\n\u001B[0;32m     40\u001B[0m     PeftModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     45\u001B[0m     PeftModelForFeatureExtraction,\n\u001B[0;32m     46\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\auto.py:31\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     22\u001B[0m     AutoModel,\n\u001B[0;32m     23\u001B[0m     AutoModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     27\u001B[0m     AutoModelForTokenClassification,\n\u001B[0;32m     28\u001B[0m )\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     33\u001B[0m     PeftModel,\n\u001B[0;32m     34\u001B[0m     PeftModelForCausalLM,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m     PeftModelForTokenClassification,\n\u001B[0;32m     40\u001B[0m )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_BaseAutoPeftModel\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\mapping.py:23\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpeft_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     24\u001B[0m     PeftModel,\n\u001B[0;32m     25\u001B[0m     PeftModelForCausalLM,\n\u001B[0;32m     26\u001B[0m     PeftModelForFeatureExtraction,\n\u001B[0;32m     27\u001B[0m     PeftModelForQuestionAnswering,\n\u001B[0;32m     28\u001B[0m     PeftModelForSeq2SeqLM,\n\u001B[0;32m     29\u001B[0m     PeftModelForSequenceClassification,\n\u001B[0;32m     30\u001B[0m     PeftModelForTokenClassification,\n\u001B[0;32m     31\u001B[0m )\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtuners\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     33\u001B[0m     AdaLoraConfig,\n\u001B[0;32m     34\u001B[0m     AdaLoraModel,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m     PromptTuningConfig,\n\u001B[0;32m     48\u001B[0m )\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _prepare_prompt_learning_config\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\peft_model.py:38\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftConfig\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtuners\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     39\u001B[0m     AdaLoraModel,\n\u001B[0;32m     40\u001B[0m     AdaptionPromptModel,\n\u001B[0;32m     41\u001B[0m     IA3Model,\n\u001B[0;32m     42\u001B[0m     LoHaModel,\n\u001B[0;32m     43\u001B[0m     LoKrModel,\n\u001B[0;32m     44\u001B[0m     LoraModel,\n\u001B[0;32m     45\u001B[0m     MultitaskPromptEmbedding,\n\u001B[0;32m     46\u001B[0m     PrefixEncoder,\n\u001B[0;32m     47\u001B[0m     PromptEmbedding,\n\u001B[0;32m     48\u001B[0m     PromptEncoder,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     51\u001B[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001B[0;32m     52\u001B[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     shift_tokens_right,\n\u001B[0;32m     65\u001B[0m )\n\u001B[0;32m     68\u001B[0m PEFT_TYPE_TO_MODEL_MAPPING \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     69\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mLORA: LoraModel,\n\u001B[0;32m     70\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mLOHA: LoHaModel,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     77\u001B[0m     PeftType\u001B[38;5;241m.\u001B[39mIA3: IA3Model,\n\u001B[0;32m     78\u001B[0m }\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\__init__.py:21\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madaption_prompt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlora\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraConfig, LoraModel\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloha\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoHaConfig, LoHaModel\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlokr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoKrConfig, LoKrModel\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py:21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgptq\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantLinear\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2d, Embedding, Linear, LoraLayer\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoraModel\n\u001B[0;32m     24\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConv2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraLayer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoraModel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantLinear\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_available():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\peft\\tuners\\lora\\model.py:45\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2d, Embedding, Linear, LoraLayer\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_available():\n\u001B[1;32m---> 45\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbnb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Linear8bitLt\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_4bit_available():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the MIT license found in the\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cuda_setup, utils, research\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     MatmulLtState,\n\u001B[0;32m      9\u001B[0m     bmm_cublas,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m     matmul_4bit\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m COMPILED_WITH_CUDA\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     switchback_bnb,\n\u001B[0;32m      4\u001B[0m     matmul_fp8_global,\n\u001B[0;32m      5\u001B[0m     matmul_fp8_mixed,\n\u001B[0;32m      6\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor, device, dtype, nn\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GlobalOptimManager\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OutlierTracer, find_outlier_dims\n\u001B[0;32m     11\u001B[0m T \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m, bound\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.nn.Module\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the MIT license found in the\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m COMPILED_WITH_CUDA\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madagrad\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madam\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\bitsandbytes\\cextension.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m     CUDASetup\u001B[38;5;241m.\u001B[39mget_instance()\u001B[38;5;241m.\u001B[39mgenerate_instructions()\n\u001B[0;32m     19\u001B[0m     CUDASetup\u001B[38;5;241m.\u001B[39mget_instance()\u001B[38;5;241m.\u001B[39mprint_log_stack()\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001B[39m\n\u001B[0;32m     22\u001B[0m \n\u001B[0;32m     23\u001B[0m \u001B[38;5;124m    python -m bitsandbytes\u001B[39m\n\u001B[0;32m     24\u001B[0m \n\u001B[0;32m     25\u001B[0m \u001B[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001B[39m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001B[39m\u001B[38;5;124m'''\u001B[39m)\n\u001B[0;32m     28\u001B[0m lib\u001B[38;5;241m.\u001B[39mcadam32bit_grad_fp32 \u001B[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001B[39;00m\n\u001B[0;32m     29\u001B[0m lib\u001B[38;5;241m.\u001B[39mget_context\u001B[38;5;241m.\u001B[39mrestype \u001B[38;5;241m=\u001B[39m ct\u001B[38;5;241m.\u001B[39mc_void_p\n",
      "\u001B[1;31mRuntimeError\u001B[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages='peft,torch,loralib,transformers,accelerate,datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b254b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T20:52:20.577266100Z",
     "start_time": "2023-11-05T20:52:20.556736900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigscience/bloom-7b1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[0;32m      3\u001B[0m     load_in_8bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[0;32m      4\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m )\n\u001B[0;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigscience/bloom-7b1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-7b1\", \n",
    "    load_in_8bit=True, \n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40e345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:23:09.815826Z",
     "start_time": "2023-05-27T04:23:09.445309Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.config\n",
    "AutoConfig.from_pretrained(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cab40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:23:32.904412Z",
     "start_time": "2023-05-27T04:23:32.896860Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bec88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:24:26.378420Z",
     "start_time": "2023-05-27T04:24:26.370376Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.transformer.word_embeddings\n",
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205279aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:24:30.082169Z",
     "start_time": "2023-05-27T04:24:30.074968Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13451a31",
   "metadata": {},
   "source": [
    "### freeze original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c024b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:26:00.858714Z",
     "start_time": "2023-05-27T04:26:00.848057Z"
    }
   },
   "outputs": [],
   "source": [
    "list(model.parameters())[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8949b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:27:01.261630Z",
     "start_time": "2023-05-27T04:27:01.200624Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "#     print(i, 'param.requires_grad = False')\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "#         print(i, 'ndim == 1, torch.float16 to torch.float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40503a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:27:30.811647Z",
     "start_time": "2023-05-27T04:27:30.802911Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce number of stored activations\n",
    "model.gradient_checkpointing_enable()  \n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510315e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:27:49.163660Z",
     "start_time": "2023-05-27T04:27:49.154423Z"
    }
   },
   "outputs": [],
   "source": [
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): \n",
    "        return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29087b4",
   "metadata": {},
   "source": [
    "### LoRa Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b202a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:12.406966Z",
     "start_time": "2023-05-27T04:28:12.400279Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860f290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:30:09.762833Z",
     "start_time": "2023-05-27T04:30:09.754680Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "config = LoraConfig(\n",
    "    r=16, #low rank\n",
    "    lora_alpha=32, #alpha scaling， scale lora weights/outputs\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83459bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:30:33.895170Z",
     "start_time": "2023-05-27T04:30:23.695760Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96eba17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:31:05.599257Z",
     "start_time": "2023-05-27T04:31:05.589168Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718a902",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac0ff3",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23e027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:32:24.503674Z",
     "start_time": "2023-05-27T04:32:20.544103Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65002e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:32:33.770607Z",
     "start_time": "2023-05-27T04:32:33.763460Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f4abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:32:48.202600Z",
     "start_time": "2023-05-27T04:32:48.194996Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d6bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:33:16.069537Z",
     "start_time": "2023-05-27T04:33:16.028446Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d23a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:33:40.816836Z",
     "start_time": "2023-05-27T04:33:40.796788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['train']['quote'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47e7ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:33:47.492377Z",
     "start_time": "2023-05-27T04:33:47.476200Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train']['author'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e31c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:34:13.386736Z",
     "start_time": "2023-05-27T04:34:13.373650Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7805c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:34:25.952405Z",
     "start_time": "2023-05-27T04:34:25.907838Z"
    }
   },
   "outputs": [],
   "source": [
    "str(dataset['train']['tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3039ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:34:54.089588Z",
     "start_time": "2023-05-27T04:34:54.073977Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(row):\n",
    "    row['prediction'] = row['quote'] + ' ->: ' + str(row['tags'])\n",
    "    return row\n",
    "dataset['train'] = dataset['train'].map(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d8575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:34:55.999771Z",
     "start_time": "2023-05-27T04:34:55.978125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['train']['prediction'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04286052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:35:21.871874Z",
     "start_time": "2023-05-27T04:35:21.864189Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f9dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:35:58.426014Z",
     "start_time": "2023-05-27T04:35:58.391819Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer(dataset['train']['prediction'][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9981d",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cdfe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:36:21.816424Z",
     "start_time": "2023-05-27T04:36:21.545578Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda samples: tokenizer(samples['prediction']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6cb7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:36:23.710570Z",
     "start_time": "2023-05-27T04:36:23.702652Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'input_ids', 'attention_mask'\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76445a9",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae43b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:36:57.623411Z",
     "start_time": "2023-05-27T04:36:57.281712Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86290120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:44:56.744479Z",
     "start_time": "2023-05-27T04:36:59.580379Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_dataset=dataset['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100, \n",
    "        max_steps=200, \n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21bd50",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6f152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:45:23.617833Z",
     "start_time": "2023-05-27T04:45:15.996301Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = tokenizer(\"“Training models with PEFT and LoRa is cool” ->: \", return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136a81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T04:45:40.138422Z",
     "start_time": "2023-05-27T04:45:33.322302Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "batch = tokenizer(\"“An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains.” ->: \", return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a36375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T03:56:04.539665Z",
     "start_time": "2023-05-27T03:56:04.534224Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.data_collator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
